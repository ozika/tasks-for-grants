{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/drive/postdoc/grants/resources/tasks-for-grants'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, os, sys \n",
    "from groo.groo import get_root\n",
    "\n",
    "os.path.join(get_root(\".tasks_root\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_bin_arr(size, no_relevant):\n",
    "\n",
    "    if no_relevant > size:\n",
    "        raise ValueError(\"Number of relevant values (1s) cannot exceed the size of the array.\")\n",
    "    \n",
    "    # Step 1: Generate the binary array\n",
    "    binary_array = np.zeros(size, dtype=int)\n",
    "    binary_array[:no_relevant] = 1\n",
    "    np.random.shuffle(binary_array)\n",
    "    \n",
    "    # Step 2: Assign sequential numbers to positions where the binary array has 1s\n",
    "    sequential_numbers = np.arange(1, no_relevant + 1)  # Generate sequential numbers starting from 1\n",
    "    np.random.shuffle(sequential_numbers)  # Shuffle the sequential numbers\n",
    "    result_array = np.zeros_like(binary_array)  # Create an array of the same shape as the binary array\n",
    "    result_array[binary_array == 1] = sequential_numbers  # Assign shuffled numbers to 1 positions\n",
    "    \n",
    "    return binary_array, result_array\n",
    "##this.trialData[this.tridx].t_trial_start = performance.now();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low1',\n",
       " 'low2',\n",
       " 'low3',\n",
       " 'low4',\n",
       " 'mid1',\n",
       " 'mid2',\n",
       " 'mid3',\n",
       " 'mid4',\n",
       " 'high1',\n",
       " 'high2',\n",
       " 'high3',\n",
       " 'high4']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conds = [c+str(i) for c in [\"low\", \"mid\", \"high\"] for i in range(1,5)] \n",
    "conds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_conditions(conds, lvls, no_relevant):\n",
    "    while True:\n",
    "        sampled_keys = random.sample(conds, no_relevant)\n",
    "        sampled_values = [lvls[key][0] for key in sampled_keys]  # Extract first values\n",
    "        \n",
    "        if (any(val > 0 for val in sampled_values) and any(val < 0 for val in sampled_values)) or (no_relevant==1):\n",
    "            return sampled_keys  # Ensure at least one positive and one negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "no_relevant = 4\n",
    "no_contexts = 6\n",
    "no_shown_t = 2\n",
    "\n",
    "reps = 1 # 8 for training # 30 for main1\n",
    "n_targets = 2\n",
    "\n",
    "noise = 2 # standard deviation \n",
    "\n",
    "\n",
    "n_stim_total = 10\n",
    "type =\"main2\" # \"train\" \"main1\" \"main2\"\n",
    "flag = \"_test\"\n",
    "\n",
    "conds = [c+str(i) for c in [\"low\", \"mid\", \"high\"] for i in range(1,5)] \n",
    "lvls = dict({\n",
    "    \"irr\": [0, 0],\n",
    "    \"low1\": [5, -15],\n",
    "    \"low2\": [-5, 15],\n",
    "    \"low3\": [15, -5],\n",
    "    \"low4\": [-15, 5],\n",
    "    \"mid1\": [25, -15], \n",
    "    \"mid2\": [-25, 15],\n",
    "    \"mid3\": [15, -25],\n",
    "    \"mid4\": [-15, 25],\n",
    "    \"high1\": [40, -20],\n",
    "    \"high2\": [-40, 20],\n",
    "    \"high3\": [20, -40],\n",
    "    \"high4\": [-20, 40]\n",
    "})\n",
    "\n",
    "for idx in range(0,5):\n",
    "    print(i)\n",
    "\n",
    "    basic_sched = pd.DataFrame()\n",
    "    ii = 1\n",
    "    for no_relevant in [no_relevant]:\n",
    "        random_keys = sample_conditions(conds, lvls, no_relevant)\n",
    "        ordered_keys = sorted(random_keys, key=lambda k: lvls[k][0], reverse=True)  # Sort by the upper bound descending\n",
    "\n",
    "        while len(ordered_keys) < no_contexts:\n",
    "            ordered_keys.append(\"irr\")\n",
    "        relevance = [\"rel\" if key != \"irr\" else \"irr\" for key in ordered_keys]\n",
    "\n",
    "        stimuli = list(range(1, n_stim_total + 1))  # Stimuli numbered from 1 to 10\n",
    "        stim_chosen = no_relevant*10 + np.array(random.sample(stimuli, no_contexts))\n",
    "\n",
    "        positions = list(range(1, 7))  # Stimuli numbered from 1 to 10\n",
    "        stim_positions = random.sample(positions, no_contexts)\n",
    "\n",
    "        correct_stim = stim_positions[0]\n",
    "        ## each rep will contain 2 trials (each for one target)\n",
    "        for r in range(reps):\n",
    "            #print(r)\n",
    "            for t in range(n_targets):\n",
    "                #for s in range()\n",
    "                shown, order = gen_bin_arr(no_contexts,no_shown_t) \n",
    "                tdf = pd.DataFrame({\"tr_id\": ii,\n",
    "                                    \"no_rel_ctxts\": no_relevant,\n",
    "                                    \"target\": t+1, \n",
    "                                    \"rel_cond\": \"rel\"+str(no_relevant),\n",
    "                                    \"relevance\": relevance, \n",
    "                                    \"condition\": ordered_keys,  \n",
    "                                    \"condition_id\": np.arange(1,no_contexts+1),\n",
    "                                    \"outcome_rel\": [lvls[x][t] for x in ordered_keys],\n",
    "                                    \"stim\": stim_chosen,  \n",
    "                                    \"stim_positions\": stim_positions,\n",
    "                                    \"shown\": shown, \n",
    "                                    \"order\": order,\n",
    "                                    \"correct_stim\": correct_stim-1, # minus 1 because it will refer to the position \n",
    "                                    \"no_shown\": no_shown_t, \n",
    "                                    \"outcome_t1\": [lvls[x][0] for x in ordered_keys],\n",
    "                                    \"outcome_t2\": [lvls[x][1] for x in ordered_keys],\n",
    "                                    })\n",
    "                a=1\n",
    "                \n",
    "                basic_sched = pd.concat([basic_sched, tdf])\n",
    "                ii = ii + 1\n",
    "            \n",
    "\n",
    "    df2 = basic_sched.copy()\n",
    "    #basic_sched.to_csv(os.path.join(get_root(\".tasks_root\"), \"contextual-inference\", \"schedules\", \"sch1_filtered.csv\"))\n",
    "\n",
    "\n",
    "    # filter and re-order schedule\n",
    "    basic_sched = basic_sched.loc[basic_sched[\"shown\"]==1,]\n",
    "\n",
    "\n",
    "    basic_sched = basic_sched.set_index([\"tr_id\", \"order\"]).unstack(\"order\")\n",
    "\n",
    "    # Flatten column multi-index\n",
    "\n",
    "    basic_sched.columns = [\n",
    "        f\"{col[0]}_stim{col[1]}\" if col[1] else col[0] for col in basic_sched.columns\n",
    "    ]\n",
    "    constant_columns = [ \"no_rel_ctxts\", \"target\", \"rel_cond\",  \"correct_stim\", \"no_shown\"] #\n",
    "\n",
    "    df_constants = df2[[\"tr_id\"] + constant_columns].drop_duplicates().set_index(\"tr_id\")\n",
    "\n",
    "    # list of columns which need to be separated \n",
    "    per_sim = [\"relevance\", \"condition\", \"condition_id\", \"outcome_rel\", \"outcome_t1\", \"outcome_t2\", \"stim\", \"stim_positions\"]\n",
    "    basic_sched = df_constants.merge(basic_sched.loc[:,[v + \"_stim\"+ str(st) for v in per_sim for st in np.arange(1,no_shown_t+1)] ], left_index=True, right_on=\"tr_id\")\n",
    "\n",
    "    # calculate total outcome\n",
    "    if type == \"train\":\n",
    "        basic_sched[\"outcome\"] = basic_sched.filter(regex=\"outcome_rel\").sum(axis=1) \n",
    "    elif (type == \"main1\") or (type == \"main2\"): \n",
    "        basic_sched[\"outcome\"] = basic_sched.filter(regex=\"outcome_rel\").sum(axis=1) + np.random.normal(0, noise,basic_sched.shape[0]).round()\n",
    "\n",
    "    # calculate hypothetical outcome for both targets\n",
    "    basic_sched[\"t1_ev\"] = basic_sched.filter(regex=\"outcome_t1_\").sum(axis=1)\n",
    "    basic_sched[\"t2_ev\"] = basic_sched.filter(regex=\"outcome_t2_\").sum(axis=1)\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    basic_sched = shuffle(basic_sched)\n",
    "\n",
    "\n",
    "\n",
    "    ## # add whether stim is shown or not\n",
    "    if type == \"train\":\n",
    "        rating_sections = [0.65] # breaks\n",
    "        rating_freqs = [0,1] # proportion of 1s in each section \n",
    "    elif (type == \"main1\") or (type == \"main2\"): \n",
    "        rating_sections = [0.3, 0.6] # breaks\n",
    "        rating_freqs = [0,0.2, 0.7] # proportion of 1s in each section \n",
    "\n",
    "    basic_sched = basic_sched.reset_index()\n",
    "    # Determine section boundaries based on index positions\n",
    "    split_indices = [int(basic_sched.shape[0] * prop) for prop in rating_sections] + [basic_sched.shape[0]]\n",
    "    # Assign binomial values based on position\n",
    "    basic_sched['require_rating'] = np.nan  # Initialize column\n",
    "    start_idx = 0\n",
    "    for i, end_idx in enumerate(split_indices):\n",
    "        basic_sched.iloc[start_idx:end_idx, basic_sched.columns.get_loc('require_rating')] = np.random.binomial(1, rating_freqs[i], size=(end_idx - start_idx))\n",
    "        start_idx = end_idx\n",
    "    # Convert binomial column to integer\n",
    "    basic_sched['require_rating'] = basic_sched['require_rating'].astype(int)\n",
    "\n",
    "\n",
    "    ### A choice trial - at least one feature has to be relevant\n",
    "    basic_sched[\"suit_for_dec\"] = basic_sched.filter(regex=\"relevance_\").apply(lambda row: 1 if \"rel\" in row.values else 0, axis=1)\n",
    "    if type == \"train\":\n",
    "        reward_tr_sections = [0.65] # breaks\n",
    "        reward_tr_rating_freqs = [0,0.4] # proportion of 1s in each section \n",
    "    elif (type == \"main1\") or (type == \"main2\"):  \n",
    "        reward_tr_sections = [0.2,0.5] # breaks\n",
    "        reward_tr_freqs = [0,0.25, 0.6] # proportion of 1s in each section \n",
    "\n",
    "    basic_sched = basic_sched.reset_index()\n",
    "    # Determine section boundaries based on index positions\n",
    "    split_indices = [int(basic_sched.shape[0] * prop) for prop in reward_tr_sections] + [basic_sched.shape[0]]\n",
    "    # Assign binomial values based on position\n",
    "    basic_sched['decision'] = np.nan  # Initialize column\n",
    "    start_idx = 0\n",
    "    for i, end_idx in enumerate(split_indices):\n",
    "        basic_sched.iloc[start_idx:end_idx, basic_sched.columns.get_loc('decision')] = np.random.binomial(1, reward_tr_freqs[i], size=(end_idx - start_idx))\n",
    "        start_idx = end_idx\n",
    "    # Convert binomial column to integer\n",
    "    basic_sched['decision'] = basic_sched['decision'].astype(int)\n",
    "    basic_sched['decision'] = basic_sched['decision']*basic_sched[\"suit_for_dec\"]\n",
    "\n",
    "    ## corect stim\n",
    "    basic_sched[\"correct_stimuli\"] = [list(basic_sched.loc[basic_sched[\"relevance_stim1\"]==\"rel\", \"condition_id_stim1\" ].unique()-1)] * basic_sched.shape[0]\n",
    "    \n",
    "\n",
    "    basic_sched.to_csv(os.path.join(get_root(\".tasks_root\"), \"contextual-inference\", \"schedules\", \"sch_r\"+str(no_relevant)+\"_\"+type+\"_\"+str(idx)+flag+\".csv\"))\n",
    "#basic_sched.to_csv(os.path.join(get_root(\".tasks_root\"), \"contextual-inference\", \"schedules\", \"sch_r\"+str(no_relevant)+\"_train2_test.csv\"))\n",
    "#df2.to_csv(os.path.join(get_root(\".tasks_root\"), \"contextual-inference\", \"schedules\", \"sch1_full.csv\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
